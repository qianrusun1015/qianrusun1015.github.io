<!doctype html>
<html>

<head>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta charset="utf-8">
	<title>Qianru Sun</title>
	<link rel="stylesheet" href="css/bootstrap.css">
	<script src="js/jquery-1.12.1.min.js"></script>
	<script src="js/bootstrap.js"></script>

	<style type="text/css">
		a:link { text-decoration:underline;color: black}
		a:active { text-decoration:blink}
		a:hover { text-decoration:underline;color: black} 
		a:visited { text-decoration:underline;color: black}
	</style>
</head>


<body>
	<div class="container">
		<ul class="nav nav-tabs">
			<li class="active"><a data-toggle="tab" href="#home">Home</a></li>
			<li><a data-toggle="tab" href="#publications">Publications</a></li>
			<li><a data-toggle="tab" href="#teaching">Teaching</a></li>
			<li><a data-toggle="tab" href="#people">Students & Staffs</a></li>
			<li><a data-toggle="tab" href="#others">Others</a></li>
		</ul>
		<div class="tab-content">
			<div id="home" class="tab-pane fade in active">
				<div class="row">
					<div class="col-sm-3">
						<br>
						<img src="images/qianru.jpg" alt="profile" style="max-width: 80%; height: auto;"/>
						<h2> Qianru Sun </h2>
						<h2> 孙倩茹 </h2>
						<h4> Assistant Professor </h4>
						<h4> Lee Kong Chian Fellow </h4>
						
						<a href="https://scis.smu.edu.sg">School of Computing and Information Systems</a>
						<br /><a href="https://www.smu.edu.sg">Singapore Management University</a>
						
						<h3>Contact Info:</h3>
						<span class="flex flex-row justify-left items-center p-4 py-1">    
							<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="#5A005B" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
								<path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path>
								<polyline points="22,6 12,13 2,6"></polyline>
							</svg>
							<span class="text-purple-500 font-semibold text-xs pl-2">
								<a href="mailto: qianrusun@smu.edu.sg"><i class="fa fa-envelope"></i> &nbsp;qianrusun AT smu DOT edu DOT sg</a>
							</span>
						</span>
						<br>
						<span class="flex flex-row justify-left items-center p-4 py-1">     
							<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="#5A005B" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
								<polygon points="1 6 1 22 8 18 16 22 23 18 23 2 16 6 8 2 1 6"></polygon>
								<line x1="8" y1="2" x2="8" y2="18"></line><line x1="16" y1="6" x2="16" y2="22"></line>
							</svg>
							<span class="text-purple-500 font-semibold text-xs pl-2">
									<a href="https://www.google.com/maps/place/80+Stamford+Rd,+Singapore+178902/@1.2973205,103.8474257,17z/data=!3m1!4b1!4m5!3m4!1s0x31da19a343c43671:0xaf1844ca89792f82!8m2!3d1.2973205!4d103.8496144" target="_blank"><i class="fa fa-globe"></i> &nbsp;80 Stamford Rd, Singapore, 178902</a>
							</span>
						</span>

						<div style="display:inline-block;width:200px;"><script type="text/javascript" src="//rf.revolvermaps.com/0/0/7.js?i=54gvhzjnmk3&amp;m=0&amp;c=007eff&amp;cr1=ff0000&amp;br=13&amp;sx=0&amp;ds=10" async="async"></script></div>
					</div>
					
					<div class="col-sm-9">
						<h2>Bio</h2>
						<p class="text-justify">I am an assistant professor of computer science in the School of Computing and Information Systems (SCIS), Singapore Management University (SMU). <a href="https://www.smu.edu.sg/faculty/profile/162151/SUN-Qianru">Here</a> is my faculty profile. From 2018 to 2019, I was a research fellow working with <a href="https://www.chuatatseng.com/">Prof. Tat-Seng Chua</a> at the National University of Singapore and <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele/">Prof.Dr. Bernt Schiele</a> at the MPI for Informatics. From 2016 to 2018, I held the Lise Meitner Award Fellowship and worked with Prof.Dr. Bernt Schiele and <a href="https://scalable.mpi-inf.mpg.de/">Prof. Dr. Mario Fritz</a> at the MPI for Informatics. I got my Ph.D. degree from Peking University in 2016 and my thesis was advised by <a href="http://robotics.pkusz.edu.cn/team/">Prof. Hong Liu</a>. In 2014, I visited in the research group of <a href="https://www.mi.t.u-tokyo.ac.jp/harada/">Prof. Tatsuya Harada</a> at the University of Tokyo. My research interests are computer vision and machine learning. 
						</p>
						
				
						
						<h2>News</h2>
						<table class="table-striped">
						<tr><td><li>
							<!-- <span class="badge badge-success" style="background-color:green;">NEW</span> -->
							<strong> 
								We are looking for PhD applicants in the Industrial Postgraduate Programme (IPP), supported by the Economic Development Board (EDB). <a href="https://graduatestudies.smu.edu.sg/phd/industrial-postgraduate-programme">[link]</a>
							</strong>
						</li></td></tr>
						<tr><td><li>
							<!-- <span class="badge badge-success" style="background-color:green;">NEW</span> -->
							<strong> Our paper about weakly-supervised semantic segmentation is accepted to CVPR '22. </strong>
						</li></td></tr>
						<tr><td><li>
							<!-- <span class="badge badge-success" style="background-color:green;">NEW</span> -->
							I am awarded "outstanding reviewer" by NeurIPS '21.  
						</li></td></tr>
						<tr><td><li>
							Two papers respectively about self-supervised learning and class-incremental learning are accepted to NeurIPS '21.
						</li></td></tr>
						<tr><td><li>
							Three papers respectively about causal attention, domain adaptation and semantic segmentation are accepted to ICCV '21.
						</li></td></tr>
						<tr><td><li>
							Our paper about food image segmentation is accepted to ACM Multimedia '21. <a href="https://xiongweiwu.github.io/foodseg103.html">[project]
						</li></td></tr>
						<tr><td><li>
							I am awarded "Lee Kong Chian Fellow" by SMU.
						</li></td></tr>
						<tr><td><li>
							We release a large-scale benchmark for food image segmentation with our pre-trained models using CNN and ViT! <a href="https://xiongweiwu.github.io/foodseg103.html">[project]
						</li></td></tr>
						<tr><td><li>
							I am awarded "outstanding reviewer" by ICLR '21.
						</li></td></tr>
						<tr><td><li>
							FoodAI++, a demo of our food image segmentation, is now online. <a href="https://foodaiseg.org/">[demo]
						</li></td></tr>
						<tr><td><li>
							Two papers respectively about incremental learning and zero-shot learning are accepted to CVPR '21.
						</li></td></tr>
						<tr><td><li>
							The 1st workshop of Causality in Vision at CVPR '21. The best paper is awarded a US$1,000 (cash) prize. <a href="http://www.causalityinvision.com/">[homepage]
						</li></td></tr>
						<tr><td><li>
							Two papers respectively about semantic segmentation and few-shot learning are accepted to NeurIPS '20.
						</li></td></tr>
						<tr><td><li>
							The extended paper of our CVPR'19 work (MTL) is accepted to IEEE Transactions on PAMI.
						</li></td></tr>
						<tr><td><li>
							Two papers respectively about semantic segmentation and few-shot learning are accepted to ECCV '20.
						</li></td></tr>
						<tr><td><li>
							Our paper about the application of teacher-student networks is accepted to IJCAI '20.
						</li></td></tr>
						<tr><td><li>
							We release the code of E3BM (SOTA few-shot learning results and LITTLE overhead costs)! <a href="https://github.com/yaoyao-liu/E3BM">[github]
						</li></td></tr>
						<tr><td><li>
							We release the code of Mnemonics Training (SOTA multi-class incremental learning results on ImageNet)! <a href="https://github.com/yaoyao-liu/mnemonics">[github]
						</li></td></tr>
						<tr><td><li>
							We release the code of VC R-CNN (SOTA image representation on MS-COCO Detection and Open Images)! <a href="https://github.com/Wangt-CN/VC-R-CNN">[github]
						</li></td></tr>
						<tr><td><li>
							Two papers respectively about incremental learning (oral presentation) and unsupervised learning are accepted to CVPR '20.
						</li></td></tr>
						<tr><td><li>
							We will host the ACM Multimedia Asia '20 conference in Singapore! <a href="http://www.mmasia2020.org/">[homepage]
						</li></td></tr>
						<tr><td><li>
							An article about my research is posted on "Research at SMU Nov 2019 Issue". <a href="https://research.smu.edu.sg/news/2019/nov/19/professor-and-her-learning-machines">[link]
						</li></td></tr>
						<tr><td><li>
							Our paper about semi-supervised few-shot learning is accepted to NeurIPS '19.
						</li></td></tr>
						<tr><td><li>
							Our paper about mixed-dish image recognition is accepted to ACM Multimedia '19.
						</li></td></tr>
						<tr><td><li>
							Our paper about visual relationship feature augmentation is accepted to BMVC '19.
						</li></td></tr>
						<tr><td><li>
							Our paper about few-shot learning is accepted to CVPR '19.
						</li></td></tr>
						<tr><td>
						</td></tr>
						</table>
					</div>
				</div>
			</div>
			<div id="people" class="tab-pane fade">
				<!-- Research Staffs -->
				<div class="container phd">
					<h3><b>Ph.D. Students</b></h3>
					<div class="row profile">
						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/yaoyao.jpg" width="120px" height="auto" alt="" class="stu-img"></div>
								<div class="col-md-6">
									<b>
										<a href="https://people.mpi-inf.mpg.de/~yaliu/">Yaoyao Liu</a>
										<br> Since Jun 2018
										<br> (with Prof. Bernt Schiele)
										<br> MPI for Informatics 
										<a href="mailto:yaoyao.liu@mpi-inf.mpg.de">yaoyao.liu[at]mpi-inf.mpg.de</a>
										<br>
									</b>
								</div>
							</div>
						</div>
			
						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/sicheng.jpg" width="120px" height="120px" alt="" lass="stu-img"></div>
								<div class="col-md-6">
									<b>
										<a href="https://scholar.google.com/citations?user=seCkFT8AAAAJ&hl=zh-CN">Sicheng Yu</a>
										<br> Since Sep 2019
										<br> (with Prof. Jing Jiang)
										<br> SMU
										<br><a href="mailto:scyu.2018@phdcs.smu.edu.sg">scyu.2018[at]phdcs.smu.edu.sg</a>
									</b>
								</div>
							</div>
						</div>
						
						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/zhaozheng.jpg" width="120px" height="120px" alt=""
										class="stu-img"></div>
								<div class="col-md-6">
									<b>
										<a href="https://zhaozhengchen.github.io/">Zhaozheng Chen</a>
										<br> Since Jan 2020
										<br> SMU
										<br><a href="mailto:zhaozhengcc@gmail.com">zhaozhengcc[at]gmail.com</a>
									</b>
								</div>
							</div>
						</div>
			
					</div>
					<br><br>
					<div class="row profile">
						
						 <div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/wangqing.jpg" width="120px" height="120px" alt="" class="stu-img"></div>
								<div class="col-md-6">
									<b>
										Qing Wang
										<br> Since Jan 2021
										<br> (with Prof. Chong-Wah Ngo)
										<br> SMU
										<br><a href="mailto:qingwang.2020@phdcs.smu.edu.sg">qingwang.2020[at]phdcs.smu.edu.sg</a>
									</b>
								</div>
							</div>
						</div>
						
						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/zilin.jpg" width="120" height="120px" alt="" class="stu-img"></div>
								<div class="col-md-6">
									<b>
										<a href="https://github.com/xfflzl/">Zilin Luo</a>
										<br> Since Aug 2021
										<br> SMU
										<a href="mailto:zilin.luo.2021@phdcs.smu.edu.sg"> zilin.luo.2021[at]phdcs.smu.edu.sg</a>
									</b>
								</div>
							</div>
						</div>
			
			
					</div>
					

				</div>
				
				<br>
				<hr>
				<br>
			
				<div class="container master">
					<h3><b>Master Students</b></h3>
			
					<div class="row profile">
			
						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/chunhui.jpg" width="120px" height="120px" alt="" class="stu-img"></div>
								<div class="col-md-6">
									<b>
										<a href="">Chunhui Bao</a>
										<br> Jan 2021-Dec 2021
										<br> SMU
										<!-- <br> <a href="mailto:xxx@xxx">xxx[at]xxx</a> -->
										<br>
									</b>
								</div>
							</div>
						</div>
						
						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/lets-talk-teeth-1.jpeg" width="120px" height="120px" alt="" class="stu-img"></div>
								<div class="col-md-6">
									<b>
										LOH Yi Lin
										<br> Jan 2022-Jun 2022
										<br> SMU
										<!-- <a href="mailto:xxx@xxx ">xxx[at]xxx</a> -->
										<br>
									</b>
								</div>
							</div>
						</div>
			
			
					</div>
			
				</div>
				
				<br>
				<hr>
				<br>
			
				<div class="container fellow">
					<h3><b>Research Fellows/Assistants</b></h3>
			
					<div class="row profile">
			
						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/lets-talk-teeth-1.jpeg" width="120px" height="120px" alt="" class="stu-img"></div>
								<div class="col-md-6">
									<b>
										Xin Fu
										<br> Jan 2020-Dec 2020
										<br> Research Assistant
										<br> Beijing Jiaotong University
										<!-- <br><a href="mailto:xxx">xxx[at]xxx</a> -->
										<br>
										
									</b>
								</div>
							</div>
						</div>

						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/qinwei.jpg" width="120px" height="120px" alt="" class="stu-img"></div>
								<div class="col-md-6">
									<b>
										Wei Qin
										<br> Nov 2019-May 2021
										<br> Research Assistant
										<br> Hefei University of Technology
										<!-- <br><a href="mailto:xxx">xxx[at]xxx</a> -->
										<br>
										
									</b>
								</div>
							</div>
						</div>
						
						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/lets-talk-teeth-1.jpeg" width="120px" height="120px" alt="" class="stu-img"></div>
								<div class="col-md-6">
									<b>
										Muhammad Naufal
										<br> Aug 2020-Dec 2020
										<br> Research Student
										<br> SMU
										<!-- <a href="mailto:xxx">xxx[at]xxx</a> -->
									</b>
								</div>
							</div>
						</div>
					</div>
					<br><br>
					<div class="row profile">
						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/lets-talk-teeth-1.jpeg" width="120px" height="120px" class="vice-fac-img"></div>
								<div class="col-md-6">
									<b>
										Ying Liu
										<br> Aug 2020-Mar 2021
										<br> Research Assistant
										<br> SMU
										<!-- <br><a href="mailto:xxx">xxx[at]xxx</a> -->
									</b>
								</div>
							</div>
						</div>

						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/xiongwei.png" width="120px" height="120px" class="vice-fac-img"></div>
								<div class="col-md-6">
									<b>
										<a href="http://xiongweiwu.github.io/">Xiongwei Wu</a>
										<br> Since Mar 2021
										<br> PostDoc
										<br> (with Prof. Ee-Peng Lim)
										<br> SMU
										<br><a href="mailto:xwwu@smu.edu.sg">xwwu[at]smu.edu.sg</a>
									</b>
								</div>
							</div>
						</div>

						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/zhaoxin.jpg" width="120px" height="120px" class="vice-fac-img"></div>
								<div class="col-md-6">
									<b>
										Xin Zhao
										<br> Jun 2021-May 2022
										<br> Visiting
										<br> Jilin University
										<!-- <br><a href="mailto:xxx">xxx[at]xxx</a> -->
									</b>
								</div>
							</div>
						</div>
					</div>
					<br><br>
					<div class="row profile">
						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/lets-talk-teeth-1.jpeg" width="120px" height="120px" class="vice-fac-img"></div>
								<div class="col-md-6">
									<b>
										Harshit Jain
										<br> Aug 2021-Dec 2021
										<br> Research Student
										<br> SMU
										<!-- <br><a href="mailto:xxx">xxx[at]xxx</a> -->
									</b>
								</div>
							</div>
						</div>

						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/fengyunwang.jpeg" width="120px" height="120px" class="vice-fac-img"></div>
								<div class="col-md-6">
									<b>
										Fengyun Wang
										<br> Nov 2021-Oct 2022
										<br> Research Assistant
										<br> Nanjing University of Science and Technology
										<!-- <br><a href="mailto:xxx">xxx[at]xxx</a> -->
									</b>
								</div>
							</div>
						</div>

						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/hanning.png" width="120px" height="120px" class="vice-fac-img"></div>
								<div class="col-md-6">
									<b>
										Ning Han
										<br> Nov 2021-Oct 2022
										<br> Visiting 
										<br> (with Prof. Ee-Peng Lim)
										<br> Hunan University
										<!-- <br><a href="mailto:xxx">xxx[at]xxx</a> -->
									</b>
								</div>
							</div>
						</div>
					</div>
					<br><br>
					<div class="row profile">
						<div class="col-md-4 col-sm-6 col-xs-12">
							<div class="row">
								<div class="col-md-5"><img src="images/khai-loong-pic.png" width="120px" height="120px" class="vice-fac-img"></div>
								<div class="col-md-6">
									<b>
										AW Khai Loong
										<br> Jan 2022-May 2022
										<br> Research Assistant
										<br> SMU
										<!-- <br><a href="mailto:xwwu@smu.edu.sg">xwwu[at]smu.edu.sg</a> -->
									</b>
								</div>
							</div>
						</div>

					</div>
				</div>

				<br>
				<hr>
				<br>

				<!-- <div class="row">
					<div class="col-sm-8">
						<h3> PhD Students </h3>
							<li> Aug 2021 - Now &nbsp;&nbsp;  Zilin Luo, Singapore Management University </li>
							<li> Jan 2021 - Now &nbsp;&nbsp;  Qing Wang, Singapore Management University (with <a href="https://scholar.google.com/citations?user=HM39HrUAAAAJ&hl=en">Prof. Chong-Wah Ngo</a>) </li>
							<li> Jan 2020 - Now &nbsp;&nbsp;  <a href="https://zhaozhengchen.github.io/">Zhaozheng Chen</a>, Singapore Management University </li>
							<li> Sep 2019 - Now  &nbsp;&nbsp;  <a href="https://scholar.google.com/citations?user=seCkFT8AAAAJ&hl=zh-CN">Sicheng Yu</a>, Singapore Management University (with <a href="https://scholar.google.com.sg/citations?user=hVTK2YwAAAAJ&hl=en">Prof. Jing Jiang</a>) </li>
							<li>Jun 2018 - Now &nbsp;&nbsp;   <a href="https://people.mpi-inf.mpg.de/~yaliu/">Yaoyao Liu</a>, MPI for Informatics (with Prof.Dr. Bernt Schiele) </li>
						<h3> Master Students </h3>
							<li> Jan 2022 - Jun 2022 &nbsp;&nbsp;  LOH Yi Lin, Singapore Management University </li>
							<li> Jan 2020 - Dec 2021 &nbsp;&nbsp;  Chunhui Bao, Singapore Management University </li>

						<h3> Research Fellows/Assistants </h3>
							<li> Jan 2022 - May 2022 &nbsp;&nbsp;  AW Khai Loong, Singapore Management University </li>
							<li> Nov 2021 - Oct 2022 &nbsp;&nbsp;  Ning Han, Hunan University (with <a href="https://scholar.google.com/citations?user=r0wOAikAAAAJ&hl=en">Prof. Ee-Peng Lim</a>)</li>
							<li> Nov 2021 - Oct 2022 &nbsp;&nbsp;  Fengyun Wang, Nanjing University of Science and Technology </li>
							<li> Aug 2021 - Dec 2021 &nbsp;&nbsp;  Harshit Jain, Singapore Management University </li>
							<li> Jun 2021 - May 2022 &nbsp;&nbsp;  Xin Zhao, Jilin University </li>
							<li> Mar 2021 - Now &nbsp;&nbsp;  <a href="http://xiongweiwu.github.io/">Dr. Xiongwei Wu</a>, Singapore Management University (with Prof. Ee-Peng Lim) </li>
							<li> Aug 2020 - Mar 2021 &nbsp;&nbsp;  Ying Liu, Singapore Management University </li>
							<li> Aug 2020 - Dec 2020 &nbsp;&nbsp;  Muhammad Naufal, Singapore Management University </li>
							<li> Nov 2019 - May 2021 &nbsp;&nbsp;  Wei Qin, Hefei University of Technology </li>
							<li> Jan 2020 - Dec 2020 &nbsp;&nbsp;  Xin Fu, Beijing Jiaotong University </li>
					</div>
				</div> -->
			</div>
			<div id="publications" class="tab-pane fade">
				<h3> Selected Publications </h3>
				<br>
				<h4> 2022 </h4>
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tbody>
						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/cvpr22_recam.png" alt="CVPR2022_ReCAM" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation </strong> <br>
								Zhaozheng Chen, Tan Wang, Xiongwei Wu, Xian-Sheng Hua, Hanwang Zhang, <strong>Qianru Sun</strong> <br>
								2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR '22. <br>
								<a href="https://arxiv.org/pdf/2203.00962">[paper]</a> <a href="https://github.com/zhaozhengChen/ReCAM">[code]</a>
								<p> </p>
								<p> Extracting class activation maps (CAM) is arguably the most standard step of generating pseudo masks for weakly supervised semantic segmentation (WSSS). Yet, we ﬁnd that the crux of the unsatisfactory pseudo masks is the binary cross-entropy loss (BCE) widely used in CAM. Speciﬁcally, due to the sum-over-class pooling nature of BCE, each pixel in CAM may be responsive to multiple classes co-occurring in the same receptive ﬁeld.  To this end, we introduce an embarrassingly simple yet surprisingly effective method: Reactivating the converged CAM with BCE by using softmax crossentropy loss (SCE), dubbed ReCAM. Given an image, we use CAM to extract the feature pixels of each single class, and use them with the class label to learn another fully-connected layer (after the backbone) with SCE. Once converged, we extract ReCAM in the same way as in CAM. </p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/aaai22_red.png" alt="AAAI2022_RED" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Deconfounded Visual Grounding </strong><br />
								Jianqiang Huang, Yu Qin, Jiaxin Qi, <strong>Qianru Sun</strong>, Hanwang Zhang <br />
								The 36th AAAI Conference on Artificial Intelligence, AAAI '22. (15%) <br />
								<a href="https://arxiv.org/abs/2112.15324">[paper]</a>
								<a href="https://github.com/JianqiangH/Deconfounded_VG ">[code]</a>
								<p>We focus on the confounding bias between language and location in the visual grounding pipeline, where we find that the bias is the major visual reasoning bottleneck. For example, the grounding process is usually a trivial languagelocation association without visual reasoning, e.g., grounding any language query containing sheep to the nearly central regions, due to that most queries about sheep have groundtruth locations at the image center. First, we frame the visual grounding pipeline into a causal graph, which shows the causalities among image, query, target location and underlying confounder. Through the causal graph, we know how to break the grounding bottleneck: deconfounded visual grounding. Second, to tackle the challenge that the confounder is unobserved in general, we propose a confounder-agnostic approach called: Referring Expression Deconfounder (RED), to remove the confounding bias. Third, we implement RED as a simple language attention, which can be applied in any grounding method. </p>
							</td>
						</tr>

					</tbody>
				</table>

				<h4> 2021 </h4>
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tbody>
						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/neurips21_ipirm.png" alt="NeurIPS2021_IPIRM" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Self-Supervised Learning Disentangled Group Representation as Feature </strong><br/>
								Tan Wang, Zhongqi Yue, Jianqiang Huang, <strong>Qianru Sun</strong>, Hanwang Zhang <br />
								2021 Conference on Neural Information Processing Systems, NeurIPS '21. (Spotlight Presentation, 3%) <br />
								<a href="https://arxiv.org/abs/2110.15255">[paper]</a>
								<a href="https://github.com/Wangt-CN/IP-IRM">[code]</a>
								<p> A good visual representation is an inference map from observations (images) to features (vectors) that faithfully reflects the hidden modularized generative factors (semantics). In this paper, we formulate the notion of "good" representation from a group-theoretic view using Higgins' definition of disentangled representation, and show that existing Self-Supervised Learning (SSL) only disentangles simple augmentation features such as rotation and colorization, thus unable to modularize the remaining semantics. To break the limitation, we propose an iterative SSL algorithm: Iterative Partition-based Invariant Risk Minimization (IP-IRM), which successfully grounds the abstract semantics and the group acting on them into concrete contrastive learning. At each iteration, IP-IRM first partitions the training samples into two subsets that correspond to an entangled group element. Then, it minimizes a subset-invariant contrastive loss, where the invariance guarantees to disentangle the group element. We prove that IP-IRM converges to a fully disentangled representation and show its effectiveness on various benchmarks. </p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/neurips21_rmm.png" alt="NeurIPS2021_RMM" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>RMM: Reinforced Memory Management for Class-Incremental Learning </strong><br />
								Yaoyao Liu, Bernt Schiele, <strong>Qianru Sun</strong> <br />
								2021 Conference on Neural Information Processing Systems, NeurIPS '21. <br />
								<a href="https://papers.nips.cc/paper/2021/file/1cbcaa5abbb6b70f378a3a03d0c26386-Paper.pdf">[paper]</a>
								<a href="https://gitlab.mpi-klsb.mpg.de/yaoyaoliu/rmm/">[code]</a>
								<p> Class-Incremental Learning (CIL) [40] trains classifiers under a strict memory
									budget: in each incremental phase, learning is done for new data, most of which
									is abandoned to free space for the next phase. The preserved data are exemplars
									used for replaying. However, existing methods use a static and ad hoc strategy
									for memory allocation, which is often sub-optimal. In this work, we propose a
									dynamic memory management strategy that is optimized for the incremental phases
									and different object classes. We call our method reinforced memory management
									(RMM), leveraging reinforcement learning. RMM training is not naturally compatible with CIL as the past, and future data are strictly non-accessible during
									the incremental phases. We solve this by training the policy function of RMM
									on pseudo CIL tasks, e.g., the tasks built on the data of the 0-th phase, and then
									applying it to target tasks. RMM propagates two levels of actions: Level-1 determines how to split the memory between old and new classes, and Level-2
									allocates memory for each specific class. In essence, it is an optimizable and
									general method for memory management that can be used in any replaying-based
									CIL method. </p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/iccv21_caam.png" alt="ICCV2021_CaaM" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Causal Attention for Unbiased Visual Recognition </strong><br />
								Tan Wang, Chang Zhou, <strong>Qianru Sun</strong>, Hanwang Zhang <br />
								International Conference on Computer Vision, ICCV '21. <br />
								<a href="https://arxiv.org/pdf/2108.08782">[paper]</a>
								<a href="https://github.com/Wangt-CN/CaaM">[code]</a>
								<p>Attention module does not always help deep models learn causal features that are robust in any confounding context, e.g., a foreground object feature is invariant to different backgrounds. This is because the confounders trick the attention to capture spurious correlations that benefit the prediction when the training and testing data are IID; while harm the prediction when the data are OOD. The sole fundamental solution to learn causal attention is by causal intervention, which requires additional annotations of the confounders, e.g., a "dog" model is learned within  "grass+dog" and "road+dog" respectively, so the "grass" and "road" contexts will no longer confound the "dog" recognition. However, such annotation is not only prohibitively expensive, but also inherently problematic, as the confounders are elusive in nature. In this paper, we propose a causal attention module (CaaM) that  self-annotates the confounders in unsupervised fashion. In particular, multiple CaaMs can be stacked and integrated in conventional attention CNN and self-attention Vision Transformer. In OOD settings, deep models with CaaM outperform those without it significantly; even in IID settings, the attention localization is also improved by CaaM, showing a great potential in applications that require robust visual saliency.</p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/iccv21_tcm.png" alt="ICCV2021_TCM" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Transporting Causal Mechanisms for Unsupervised Domain Adaptation </strong><br />
								Zhongqi Yue, <strong>Qianru Sun</strong>, Xian-Sheng Hua, Hanwang Zhang <br />
								International Conference on Computer Vision, ICCV '21. (Oral Presentation, 3%) <br />
								<a href="https://arxiv.org/pdf/2107.11055">[paper]</a>
								<a href="https://github.com/yue-zhongqi/tcm">[code]</a>
								<p> Existing Unsupervised Domain Adaptation (UDA) literature adopts the covariate shift and conditional shift assumptions, which essentially encourage models to learn common features across domains. However, due to the lack of supervision in the target domain, they suffer from the semantic loss: the feature will inevitably lose nondiscriminative semantics in source domain, which is however discriminative in target domain. We use a causal view—transportability theory —to identify that such loss is in fact a confounding effect, which can only be removed by causal intervention. However, the theoretical solution provided by transportability is far from practical for UDA, because it requires the stratification and representation of the unobserved confounder that is the cause of the domain gap. To this end, we propose a practical solution: Transporting Causal Mechanisms (TCM), to identify the confounder stratum and representations by using the domain-invariant disentangled causal mechanisms, which are discovered in an unsupervised fashion. </p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/iccv21_sr.png" alt="ICCV2021_SR" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Self-Regulation for Semantic Segmentation </strong><br />
								Dong Zhang, Hanwang Zhang, Jinhui Tang, Xian-Sheng Hua, <strong>Qianru Sun</strong> <br />
								International Conference on Computer Vision, ICCV '21. <br />
								<a href="https://arxiv.org/pdf/2108.09702">[paper]</a>
								<a href="https://github.com/dongzhang89/sr-ss">[code]</a>
								<p> In this paper, we seek reasons for the two major failure cases in Semantic Segmentation (SS): 1) missing small objects or minor object parts, and 2) mislabeling minor parts of large objects as wrong classes. We have an interesting finding that Failure-1 is due to the underuse of detailed features and Failure-2 is due to the underuse of visual contexts. To help the model learn a better trade-off, we introduce several Self-Regulation (SR) losses for training SS neural networks. By “self”, we mean that the losses are from the model per se without using any additional data or supervision. By applying the SR losses, the deep layer features are regulated by the shallow ones to preserve more details; meanwhile, shallow layer classification logits are regulated by the deep ones to capture more semantics. We conduct extensive experiments on both weakly and fully supervised SS tasks, and the results show that our approach consistently surpasses the baselines. </p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/cvpr21_aanets.png" alt="CVPR2021_AANets" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Adaptive Aggregation Networks for Class-Incremental Learning </strong><br />
								Yaoyao Liu, Bernt Schiele, <strong>Qianru Sun</strong> <br />
								2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR '21. <br />
								<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Adaptive_Aggregation_Networks_for_Class-Incremental_Learning_CVPR_2021_paper.pdf">[paper]</a>
								<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Liu_Adaptive_Aggregation_Networks_CVPR_2021_supplemental.pdf">[supp]</a>
								<a href="https://github.com/yaoyao-liu/class-incremental-learning/tree/main/adaptive-aggregation-networks">[code]</a>
								<p>Class-Incremental Learning (CIL) aims to learn a classification model with the number of classes increasing phase-by-phase. An inherent problem in CIL is the stability-plasticity dilemma between the learning of old and new classes, i.e., high-plasticity models easily forget old classes, but high-stability models are weak to learn new classes. We alleviate this issue by proposing a novel network architecture called Adaptive Aggregation Networks (AANets) in which we explicitly build two types of residual blocks at each residual level (taking ResNet as the baseline architecture): a stable block and a plastic block. We aggregate the output feature maps from these two blocks and then feed the results to the next-level blocks. We adapt the aggregation weights in order to balance these two types of blocks, i.e., to balance stability and plasticity, dynamically.</p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/cvpr21_gcm.png" alt="CVPR2021_GCM" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Counterfactual Zero-Shot and Open-Set Visual Recognition </strong><br />
								Zhongqi Yue, Tan Wang, <strong>Qianru Sun</strong>, Xian-Sheng Hua, Hanwang Zhang <br />
								2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR '21. <br />
								<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yue_Counterfactual_Zero-Shot_and_Open-Set_Visual_Recognition_CVPR_2021_paper.pdf">[paper]</a>
								<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Yue_Counterfactual_Zero-Shot_and_CVPR_2021_supplemental.pdf">[supp]</a>
								<a href="https://github.com/yue-zhongqi/gcm-cf">[code]</a>
								<p>We present a novel counterfactual framework for both Zero-Shot Learning and Open-Set Recognition, whose common challenge is generalizing to the unseen-classes by only training on the seen-classes. Our idea stems from the observation that the generated samples for unseen-classes are often out of the true distribution, which causes severe recognition rate imbalance between the seen-class (high) and unseen-class (low). We show that the key reason is that the generation is not Counterfactual Faithful, and thus we propose a faithful one, whose generation is from the sample-specific counterfactual question: What would the sample look like, if we set its class attribute to a certain class, while keeping its sample attribute unchanged? Thanks to the faithfulness, we can apply the Consistency Rule to perform unseen/seen binary classification, by asking: Would its counterfactual still look like itself? If "yes", the sample is from a certain class, and "no" otherwise. </p>
							</td>
						</tr>

					</tbody>
				</table>
				
				<h4> 2020 </h4>
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tbody>
						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/neurips20_conta.png" alt="NeurIPS2020_CONTA" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Causal Intervention for Weakly-Supervised Semantic Segmentation </strong><br />
								Dong Zhang, Hanwang Zhang, Jinhui Tang, Xian-Sheng Hua, <strong>Qianru Sun</strong> <br />
								Neural Information Processing Systems, NeurIPS '20. (Oral Presentation, 1.1%) <br />
								<a href="https://proceedings.neurips.cc/paper/2020/file/07211688a0869d995947a8fb11b215d6-Paper.pdf">[paper]</a>
								<a href="https://github.com/ZHANGDONG-NJUST/CONTA">[code]</a>
								<p>We present a causal inference framework to improve Weakly-Supervised Semantic Segmentation (WSSS). Specifically, we aim to generate better pixel-level pseudo-masks by using only image-level labels -- the most crucial step in WSSS. We attribute the cause of the ambiguous boundaries of pseudo-masks to the confounding context, e.g., the correct image-level classification of "horse" and "person" may be not only due to the recognition of each instance, but also their co-occurrence context, making the model inspection (e.g., CAM) hard to distinguish between the boundaries. Inspired by this, we propose a structural causal model to analyze the causalities among images, contexts, and class labels. Based on it, we develop a new method: Context Adjustment (CONTA), to remove the confounding bias in image-level classification and thus provide better pseudo-masks as ground-truth for the subsequent segmentation model. </p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/neurips20_ifsl.png" alt="NeurIPS2021_IFSL" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Interventional Few-Shot Learning </strong><br />
								Zhongqi Yue, Hanwang Zhang, <strong>Qianru Sun</strong>, Xian-Sheng Hua <br />
								Neural Information Processing Systems, NeurIPS '20.  <br />
								<a href="https://proceedings.neurips.cc/paper/2020/file/1cc8a8ea51cd0adddf5dab504a285915-Paper.pdf">[paper]</a>
								<a href="https://github.com/yue-zhongqi/ifsl">[code]</a>
								<p> We uncover an ever-overlooked deficiency in the prevailing Few-Shot Learning (FSL) methods: the pre-trained knowledge is indeed a confounder that limits the performance. This finding is rooted from our causal assumption: a Structural Causal Model (SCM) for the causalities among the pre-trained knowledge, sample features, and labels. Thanks to it, we propose a novel FSL paradigm: Interventional FewShot Learning (IFSL). Specifically, we develop three effective IFSL algorithmic implementations based on the backdoor adjustment, which is essentially a causal intervention towards the SCM of many-shot learning: the upper-bound of FSL in a causal view. It is worth noting that the contribution of IFSL is orthogonal to existing fine-tuning and meta-learning based FSL methods, hence IFSL can improve all of them, achieving a new 1-/5-shot state-of-the-art.</p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/eccv20_fpt.png" alt="ECCV2020_FPT" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Feature Pyramid Transformer </strong><br />
								Dong Zhang, Hanwang Zhang, Jinhui Tang, Meng Wang, Xian-Sheng Hua, <strong>Qianru Sun</strong> <br />
								European Conference on Computer Vision, ECCV '20.  <br />
								<a href="https://arxiv.org/pdf/2007.09451.pdf">[paper]</a>
								<a href="https://github.com/ZHANGDONG-NJUST/FPT">[code]</a>
								<p>Feature interactions across space and scales underpin modern visual recognition systems because they introduce beneficial visual contexts. Conventionally, spatial contexts are passively hidden in the CNN's increasing receptive fields or actively encoded by non-local convolution. Yet, the non-local spatial interactions are not across scales, and thus they fail to capture the non-local contexts of objects (or parts) residing in different scales. To this end, we propose a fully active feature interaction across both space and scales, called Feature Pyramid Transformer. It transforms any feature pyramid into another feature pyramid of the same size but with richer contexts, by using three specially designed transformers in self-level, top-down, and bottom-up interaction fashion. FPT serves as a generic visual backbone with fair computational overhead. </p>
							</td>
						</tr>
						
						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/eccv20_e3bm.png" alt="ECCV2020_E3BM" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>An Ensemble of Epoch-wise Empirical Bayes for Few-shot Learning </strong><br />
								Yaoyao Liu, Bernt Schiele, <strong>Qianru Sun</strong> <br />
								European Conference on Computer Vision, ECCV '20.  <br />
								<a href="https://arxiv.org/pdf/1904.08479">[paper]</a>
								<a href="https://github.com/yaoyao-liu/E3BM">[code]</a>
								<p>Few-shot learning aims to train efficient predictive models with a few examples. The lack of training data leads to poor models that perform high-variance or low-confidence predictions. In this paper, we propose to meta-learn the ensemble of epoch-wise empirical Bayes models (E3BM) to achieve robust predictions. "Epoch-wise" means that each training epoch has a Bayes model whose parameters are specifically learned and deployed. "Empirical" means that the hyperparameters, e.g., used for learning and ensembling the epoch-wise models, are generated by hyperprior learners conditional on task-specific data. We introduce four kinds of hyperprior learners by considering inductive vs. transductive, and epoch-dependent vs. epoch-independent, in the paradigm of meta-learning. Our ablation study shows that both "epoch-wise ensemble" and "empirical" encourage high efficiency and robustness in the model performance.</p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/cvpr20_mnemonics.png" alt="CVPR2020_Mnemonics" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Mnemonics Training: Multi-Class Incremental Learning without Forgetting </strong><br />
								Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, <strong>Qianru Sun</strong> <br />
								The 33rd Conference on Computer Vision and Pattern Recognition, CVPR '20. (Oral Presentation, 4%)<br />
								<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Mnemonics_Training_Multi-Class_Incremental_Learning_Without_Forgetting_CVPR_2020_paper.pdf">[paper]</a>
								<a href="http://openaccess.thecvf.com/content_CVPR_2020/supplemental/Liu_Mnemonics_Training_Multi-Class_CVPR_2020_supplemental.zip">[supp.]</a>
								<a href="https://youtu.be/zFFevd57GO4">[video]</a>
								<a href="https://github.com/yaoyao-liu/mnemonics">[code]</a>
								<p>Multi-Class Incremental Learning aims to learn new concepts by incrementally updating a model trained on previous concepts. However, there is an inherent trade-off to effectively learning new concepts without catastrophic forgetting of previous ones. To alleviate this issue, it has been proposed to keep around a few examples of the previous concepts but the effectiveness of this approach heavily depends on the representativeness of these examples. This paper proposes a novel and automatic framework we call mnemonics, where we parameterize exemplars and make them optimizable in an end-to-end manner. We train the framework through bilevel optimizations, i.e., model-level and exemplar-level. We conduct extensive experiments on three MCIL benchmarks. Interestingly and quite intriguingly, the mnemonics exemplars tend to be on the boundaries between classes.</p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/cvpr20_cvrcnn.png" alt="CVPR2020_VC-RCNN" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Visual Commonsense R-CNN </strong><br />
								Tan Wang, Jianqiang Huang, Hanwang Zhang, <strong>Qianru Sun</strong> <br />
								The 33rd Conference on Computer Vision and Pattern Recognition, CVPR '20. <br />
								<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Visual_Commonsense_R-CNN_CVPR_2020_paper.pdf">[paper]</a>
								<a href="http://openaccess.thecvf.com/content_CVPR_2020/supplemental/Wang_Visual_Commonsense_R-CNN_CVPR_2020_supplemental.pdf">[supp.]</a>
								<a href="https://www.youtube.com/watch?v=c-bpehi3KT0&feature=youtu.be">[video]</a>
								<a href="https://github.com/Wangt-CN/VC-R-CNN">[code]</a>
								<p>We present a novel unsupervised feature representation learning method, Visual Commonsense Region-based Convolutional Neural Network (VC R-CNN), to serve as an improved visual region encoder for high-level tasks such as captioning and VQA. Given a set of detected object regions in an image (e.g., by Faster R-CNN), like any other unsupervised feature learning methods (e.g., word2vec), the proxy training objective of VC R-CNN is to predict the contextual objects of a region. However, they are fundamentally different: the prediction of VC R-CNN is by causal intervention: P(Y|do(X)), while others are by the conventional likelihood: P(Y|X). This is also the core reason why VC R-CNN can learn ``sense-making'' knowledge --- like "chair" can be sat --- while not just common co-occurrences --- "chair" is likely to exist if "table" is observed.</p>
							</td>
						</tr>

					</tbody>
				</table>

				<h4> 2019 </h4>
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tbody>
						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/neurips19_lst.png" alt="NeurIPS2019_LST" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Learning to Self-Train for Semi-Supervised Few-Shot Classification </strong><br />
								Xinzhe Li,  <strong>Qianru Sun</strong>, Yaoyao Liu, Shibao Zheng, Tat-Seng Chua, Bernt Schiele <br />
								The 33rd Annual Conference on Neural Information Processing Systems, NeurIPS'19. <br />
								<a href="https://arxiv.org/pdf/1906.00562.pdf">[paper]</a>
								<a href="https://drive.google.com/file/d/151ZyvJK77nPJ36LA2gdk3S--8caXS43-/view?usp=sharing">[slides]</a>
								<a href="https://drive.google.com/file/d/1KtxOgEhaakaKod8vlvUnax8udqomGSRJ/view?usp=sharing">[poster]</a>
								<a href="https://github.com/xinzheli1217/learning-to-self-train">[code]</a>
								<p>Few-shot classification (FSC) is challenging due to the scarcity of labeled training data (e.g. only one labeled data point per class). Meta-learning has shown to achieve promising results by learning to initialize a classification model for FSC. In this paper we propose a novel semi-supervised meta-learning method called learning to self-train (LST) that leverages unlabeled data and specifically meta-learns how to cherry-pick and label such unsupervised data to further improve performance. To this end, we train the LST model through a large number of semi-supervised few-shot tasks. On each task, we train a few-shot model to predict pseudo labels for unlabeled data, and then iterate the self-training steps on labeled and pseudo-labeled data with each step followed by fine-tuning. We additionally learn a soft weighting network (SWN) to optimize the self-training weights of pseudo labels so that better ones can contribute more to gradient descent optimization.</p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/cvpr19_mtl.png" alt="CVPR2019_MTL" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Meta-Transfer Learning for Few-Shot Learning </strong><br />
								<strong>Qianru Sun</strong>, Yaoyao Liu, Tat-Seng Chua, Bernt Schiele <br />
								The 32nd Conference on Computer Vision and Pattern Recognition, CVPR'19.<br />
								<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.pdf">[paper]</a>
								<a href="https://drive.google.com/file/d/1HIupbuVs1rT-ADNQwCPBCgkPvHzH5UYa/view?usp=sharing">[poster]</a>
								<a href="https://github.com/yaoyao-liu/meta-transfer-learning">[code]</a>
								<p>Meta-learning has been proposed as a framework to address the challenging few-shot learning setting. The key idea is to leverage a large number of similar few-shot tasks in order to learn how to adapt a base-learner to a new task for which only a few labeled samples are available. As deep neural networks (DNNs) tend to overfit using a few samples only, meta-learning typically uses shallow neural networks (SNNs), thus limiting its effectiveness. In this paper we propose a novel few-shot learning method called meta-transfer learning (MTL) which learns to adapt a deep NN for few shot learning tasks. Specifically, meta refers to training multiple tasks, and transfer is achieved by learning scaling and shifting functions of DNN weights for each task. In addition, we introduce the hard task (HT) meta-batch scheme as an effective learning curriculum for MTL. </p>
							</td>
						</tr>

					</tbody>
				</table>

				<h4> 2018 </h4>
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tbody>
						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/eccv18_face.png" alt="ECCV2018_Face" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>A Hybrid Model for Identity Obfuscation by Face Replacement </strong><br />
								<strong>Qianru Sun</strong>, Ayush Tewari, Weipeng Xu, Mario Fritz, Christian Theobalt, Bernt Schiele<br/>
								The 15th European Conference on Computer Vision, ECCV'18.<br />
								<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Qianru_Sun_A_Hybrid_Model_ECCV_2018_paper.pdf">[paper]</a>
								<a href="https://github.com/qianrusun1015/Disentangled-Person-Image-Generation">[decoder code]</a>
								<p> As more and more personal photos are shared and tagged in social media, avoiding privacy risks such as unintended recognition, becomes increasingly challenging. We propose a new hybrid approach to obfuscate identities in photos by head replacement. Our approach combines state of the art parametric face synthesis with latest advances in Generative Adversarial Networks (GAN) for data-driven image synthesis. On the one hand, the parametric part of our method gives us control over the facial parameters and allows for explicit manipulation of the identity. On the other hand, the data-driven aspects allow for adding fine details and overall realism as well as seamless blending into the scene context. In our experiments we show highly realistic output of our system that improves over the previous state of the art in obfuscation rate while preserving a higher similarity to the original image content.</p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/cvpr18_dpig.png" alt="CVPR2018_DPIG" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Disentangled Person Image Generation </strong><br />
								Liqian Ma, <strong>Qianru Sun</strong>, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, Mario Fritz <br />
								2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR'18. (Spotlight Presentation)<br />
								<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_Disentangled_Person_Image_CVPR_2018_paper.pdf">[paper]</a>
								<a href="https://github.com/qianrusun1015/Disentangled-Person-Image-Generation">[code]</a>
								<a href="https://homes.esat.kuleuven.be/~liqianma/CVPR18_DPIG/">[project]</a>
								<p>Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time. First, a multi-branched reconstruction network is proposed to disentangle and encode the three factors into embedding features, which are then combined to re-compose the input image itself. Second, three corresponding mapping functions are learned in an adversarial manner in order to map Gaussian noise to the learned embedding feature space, for each factor, respectively. Using the proposed framework, we can manipulate the foreground, background and pose of the input image, and also sample new embedding features to generate such targeted manipulations, that provide more control over the generation process. Experiments on the Market-1501 and Deepfashion datasets show that our model does not only generate realistic person images with new foregrounds, backgrounds and poses, but also manipulates the generated factors and interpolates the in-between states. Another set of experiments on Market-1501 shows that our model can also be beneficial for the person re-identification task.</p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/cvpr18_head.png" alt="CVPR2018_Head" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Natural and Effective Obfuscation by Head Inpainting </strong><br />
								<strong>Qianru Sun</strong>, Liqian Ma, Seong Joon Oh, Luc Van Gool, Bernt Schiele, Mario Fritz <br />
								2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR'18. <br />
								<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Natural_and_Effective_CVPR_2018_paper.pdf">[paper]</a>
								<a href="https://github.com/qianrusun1015/Disentangled-Person-Image-Generation">[decoder code]</a>
								<a href="https://gist.github.com/charliememory/da7a22eb592231d3ea9dbaf02bcafc9c">[PDM code]</a>
								<p>As more and more personal photos are shared online, being able to obfuscate identities in such photos is becoming a necessity for privacy protection. People have largely resorted to blacking out or blurring head regions, but they result in poor user experience while being surprisingly ineffective against state of the art person recognizers. In this work, we propose a novel head inpainting obfuscation technique. Generating a realistic head inpainting in social media photos is challenging because subjects appear in diverse activities and head orientations. We thus split the task into two sub-tasks: (1) facial landmark generation from image context (e.g. body pose) for seamless hypothesis of sensible head pose, and (2) facial landmark conditioned head inpainting. We verify that our inpainting method generates realistic person images, while achieving superior obfuscation performance against automatic person recognizers.</p>
							</td>
						</tr>
					</tbody>
				</table>

				<h4> 2017 </h4>
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tbody>
						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/neurips17_pg2.png" alt="NeurIPS2017_PG2" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>Pose Guided Person Image Generation </strong><br />
								Liqian Ma, Xu Jia, <strong>Qianru Sun</strong>, Bernt Schiele, Tinne Tuytelaars, Luc Van Gool<br />
								The 31st Annual Conference on Neural Information Processing, NIPS'17. <br />
								<a href="https://papers.nips.cc/paper/6644-pose-guided-person-image-generation.pdf">[paper]</a>
								<a href="https://www.youtube.com/watch?v=FBW6drMpK3U&feature=youtu.be">[slides]</a>
								<a href="https://github.com/charliememory/Pose-Guided-Person-Image-Generation">[code]</a>
								<p>This paper proposes the novel Pose Guided Person Generation Network (PG2) that allows to synthesize person images in arbitrary poses, based on an image of that person and a novel pose. Our generation framework PG2 utilizes the pose information explicitly and consists of two key stages: pose integration and image refinement. In the first stage the condition image and the target pose are fed into a U-Net-like network to generate an initial but coarse image of the person with the target pose. The second stage then refines the initial and blurry result by training a U-Net-like generator in an adversarial way. Extensive experimental results on both 128×64 re-identification images and 256×256 fashion photos show that our model generates high-quality person images with convincing details.</p>
							</td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%;vertical-align:middle">
								<img src="paper_figure/cvpr17_social.jpeg" alt="CVPR2017_Social" style="max-width: 100%; height: auto;"/>
							</td>
							<td width="75%" valign="middle">
								<strong>A Domain Based Approach to Social Relation Recognition </strong><br />
								<strong>Qianru Sun</strong>, Bernt Schiele, Mario Fritz<br />
								2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR'17. <br />
								<a href="http://openaccess.thecvf.cocvm/content_cvpr_2017/papers/Sun_A_Domain_Based_CVPR_2017_paper.pdf">[paper]</a>
								<a href="https://github.com/qianrusun1015/social-relation-tensorflow">[code]</a>
								<a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/social-relation-recognition/">[project]</a>
								<p>Social relations are the foundation of human daily life. Developing techniques to analyze such relations from visual data bears great potential to build machines that better understand us and are capable of interacting with us at a social level. Previous investigations have remained partial due to the overwhelming diversity and complexity of the topic and consequently have only focused on a handful of social relations. In this paper, we argue that the domain-based theory from social psychology is a great starting point to systematically approach this problem. The theory provides coverage of all aspects of social relations and equally is concrete and predictive about the visual attributes and behaviors defining the relations included in each domain. We provide the first dataset built on this holistic conceptualization of social life that is composed of a hierarchical label space of social domains and social relations. We also contribute the first models to recognize such domains and relations and find superior performance for attribute based features. Beyond the encouraging performance of the attribute based approach, we also find interpretable features that are in accordance with the predictions from social psychology literature. Beyond our findings, we believe that our contributions more tightly interleave visual recognition and social psychology theory that has the potential to complement the theoretical work in the area with empirical and data-driven models of social life.</p>
							</td>
						</tr>

					</tbody>
				</table>

				<h4>Full Publications </h4>
				<div class="row">
					<div class="col-sm-3">
						<a href="https://scholar.google.de/citations?user=fNfrGMIAAAAJ&hl=en">Google Scholar Profile</a>
					</div>
					<div class="col-sm-3">
						<a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Qianru">DBLP Profile</a>
					</div>
				</div>
				<div><br></div>

			</div>
			<div id="others" class="tab-pane fade">
				</br>
				<h3> Awards and Funding </h3>
					<li> Oct 2021, Outstanding Reviewer (NeurIPS 2021)</li>
					<li> Aug 2021, Alibaba Innovative Research Grant (Alibaba Group)</li>
					<li> Jul 2021, Lee Kong Chian Fellowship (SMU)</li>
					<li> Mar 2021, Outstanding Reviewer (ICLR 2021)</li>
					<li> Nov 2020, Young Independent Research Grant (A*STAR)</li>
					<li> Feb 2020, Alibaba Innovative Research Grant (Alibaba Group)</li>
					<li> Mar 2016, Lise-Meitner Award for Excellent Women in Computer Science (MPI for Informatics)</li>
				<h3> Services </h3>
					<li> Pattern Recognition, Associate Editor</li>
					<li> AAAI'22, Meta-Reviewer; CVPR'22, Area Chair</li>
					<li> Causality in Vision @CVPR'21, Organization Committee</li>
					<li> ACM MM'21 (Chengdu), Organization Committee (Proceeding Co-Chair)</li>
					<li> ACM MM Asia'20 (Singapore), Organization Committee (Program Co-Chair)</li>
					<li> ICML'21, ICLR'21-, NeurIPS'20-, ECCV'20, IJCAI'20-, AAAI'20-, CVPR'18-, ICCV'17-, Reviewer</li>
					<li> IEEE Trans on PAMI/TMM/TCSVT/TIP/NNLS, IJCV, PR, PR Letters, Reviewer</li>
					<li> Lise Meitner Award (MPII) 2018, Organization Committee</li>
				<h3> Invited Talks </h3>
					<li> May 2022,  ICLR 2022 Workshop of Objects, Structure, and Causality (OSC). "Learning Invariance" <a href=" ">[to come]</a></li>
					<li> Jul 2020, CSIAM Big Data & AI Forum. "Learning to Learn" <a href="assets/slides/Learning_to_learn_2020-7-20.pdf">[slides]</a></li>
					<li> Jan 2018, ICMR 2018 Tutorial. "Objects, Relationships, and Context in Visual Data" <a href="https://drive.google.com/file/d/1CWFk1NLkWOwZTbJBxaGTtLOh0YzrahGK/view">[slides]</a></li>
					<li> Dec 2017, DVMM Lab at Columbia University. "Pose Guided Person Image Generation" <a href="https://www.dropbox.com/s/9m5hf1khc89b58j/talk_columbiaU.pdf?dl=0">[slides]</a></li>
					<li> Jul 2017, CVPR 2017 ODAR Workshop. "Domain Based Social Relation Recognition" <a href="https://drive.google.com/file/d/0B2-T7A3B3OlQcXhjMVZ3TFVTTUU/view">[slides]</a></li>
					<li> Jul 2017, MPII & Saarland University. "Your Photos Expose Your Social Life" <a href="https://www.dropbox.com/s/62rhop26lqq6dlu/Your%20photos%20expose%20your%20social%20life_short.pdf?dl=0">[slides]</a></li>
			</div>

			<div id="teaching" class="tab-pane fade">
			</br>
			<h3> Teaching </h3>
				<li> 2021 &nbsp;  CS701 - Deep Learning and Vision (PG) </li>
				<li> 2020-2021 &nbsp;  CS470 - UResearch Projects (UG)</li>
				<li> 2019-2021  &nbsp;  IS111 - Introduction to Programming (UG)</li>
				<li> 2020  &nbsp;  IS112 - Data Management (UG)</li>
		</div>
		</div>
	</div>
</body>

</html>
